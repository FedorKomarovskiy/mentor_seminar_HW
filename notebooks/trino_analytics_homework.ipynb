{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trino Analytics Homework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete analytics workflow using Trino for federated queries across PostgreSQL and MySQL databases, with data visualization and long-term storage in Apache Iceberg.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **Trino**: Distributed SQL query engine for federated queries\n",
    "- **PostgreSQL**: Source database containing customer and order data\n",
    "- **MySQL**: Source database containing payment data\n",
    "- **Apache Iceberg**: Modern table format for analytics with MinIO storage backend\n",
    "- **Python**: Analytics environment with pandas, matplotlib, and trino libraries\n",
    "\n",
    "### Workflow Levels\n",
    "\n",
    "1. **Level 1**: Connections and catalog exploration\n",
    "2. **Level 2**: Data aggregation and DataFrame creation\n",
    "3. **Level 3**: Visualization and Iceberg storage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all required libraries and modules for the analytics workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append('/home/jovyan/work')\n",
    "from trino_connection import (\n",
    "    create_trino_connection,\n",
    "    test_catalog_connectivity,\n",
    "    execute_sql_query,\n",
    "    get_catalog_tables,\n",
    "    test_data_access,\n",
    "    close_connection\n",
    ")\n",
    "from data_aggregation import (\n",
    "    aggregate_daily_orders,\n",
    "    aggregate_daily_payments,\n",
    "    merge_dataframes_with_fillna,\n",
    "    calculate_payment_coverage,\n",
    "    create_final_analytics_dataframe,\n",
    "    get_analytics_summary\n",
    ")\n",
    "from visualization import (\n",
    "    create_time_series_revenue_chart,\n",
    "    create_payment_coverage_histogram,\n",
    "    create_combined_analytics_dashboard,\n",
    "    display_chart_summary\n",
    ")\n",
    "from iceberg_storage import (\n",
    "    save_analytics_to_iceberg,\n",
    "    query_iceberg_table,\n",
    "    verify_data_persistence,\n",
    "    list_iceberg_tables\n",
    ")\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully\")\n",
    "print(f\"üìÖ Notebook execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Level 1: Connections and Catalog Exploration\n",
    "\n",
    "In this level, we establish connections to Trino and explore the available catalogs and data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Establish Trino Connection\n",
    "\n",
    "Create a connection to the Trino coordinator and verify connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Trino\n",
    "print(\"üîå Establishing connection to Trino...\")\n",
    "conn = create_trino_connection()\n",
    "print(\"‚úÖ Trino connection established successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Test Catalog Connectivity\n",
    "\n",
    "Verify that all configured catalogs (PostgreSQL, MySQL, Iceberg) are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connectivity to all catalogs\n",
    "print(\"üîç Testing catalog connectivity...\")\n",
    "catalog_status = test_catalog_connectivity(conn)\n",
    "\n",
    "print(\"\\nüìä Catalog Connectivity Status:\")\n",
    "print(\"=\" * 35)\n",
    "for catalog, status in catalog_status.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{status_icon} {catalog:<15}: {'Connected' if status else 'Failed'}\")\n",
    "\n",
    "successful_catalogs = sum(catalog_status.values())\n",
    "total_catalogs = len(catalog_status)\n",
    "print(f\"\\nüìà Summary: {successful_catalogs}/{total_catalogs} catalogs connected successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explore Available Catalogs\n",
    "\n",
    "List all available catalogs and their schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available catalogs\n",
    "catalogs_df = execute_sql_query(conn, \"SHOW CATALOGS\")\n",
    "print(\"üìö Available Catalogs:\")\n",
    "display(catalogs_df)\n",
    "\n",
    "print(f\"\\nTotal catalogs available: {len(catalogs_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Explore PostgreSQL Tables\n",
    "\n",
    "Examine the structure and content of PostgreSQL tables containing order and customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore PostgreSQL tables\n",
    "print(\"üêò PostgreSQL Tables:\")\n",
    "pg_tables = get_catalog_tables(conn, \"postgresql\", \"public\")\n",
    "display(pg_tables)\n",
    "\n",
    "# Sample data from key tables\n",
    "print(\"\\nüìã Sample Customer Data:\")\n",
    "customers_sample = execute_sql_query(conn, \"\"\"\n",
    "    SELECT customer_id, customer_name, email, created_at\n",
    "    FROM postgresql.public.trn_customers\n",
    "    ORDER BY customer_id\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(customers_sample)\n",
    "\n",
    "print(\"\\nüì¶ Sample Orders Data:\")\n",
    "orders_sample = execute_sql_query(conn, \"\"\"\n",
    "    SELECT order_id, customer_id, order_ts, total_amount\n",
    "    FROM postgresql.public.trn_orders\n",
    "    ORDER BY order_ts DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(orders_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Explore MySQL Tables\n",
    "\n",
    "Examine the structure and content of MySQL tables containing payment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore MySQL tables\n",
    "print(\"üê¨ MySQL Tables:\")\n",
    "mysql_tables = get_catalog_tables(conn, \"mysql\", \"demo_db\")\n",
    "display(mysql_tables)\n",
    "\n",
    "# Sample payment data\n",
    "print(\"\\nüí≥ Sample Payment Data:\")\n",
    "payments_sample = execute_sql_query(conn, \"\"\"\n",
    "    SELECT payment_id, order_id, amount, paid_at, payment_method\n",
    "    FROM mysql.demo_db.trn_payments\n",
    "    ORDER BY paid_at DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(payments_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Test Cross-Catalog Queries\n",
    "\n",
    "Demonstrate Trino's federated query capabilities by joining data across PostgreSQL and MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cross-catalog join\n",
    "print(\"üîó Cross-Catalog Join Example:\")\n",
    "cross_catalog_query = \"\"\"\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.customer_id,\n",
    "    o.total_amount as order_amount,\n",
    "    p.amount as paid_amount,\n",
    "    p.payment_method,\n",
    "    o.order_ts,\n",
    "    p.paid_at,\n",
    "    CASE \n",
    "        WHEN p.amount >= o.total_amount THEN 'Fully Paid'\n",
    "        WHEN p.amount > 0 THEN 'Partially Paid'\n",
    "        ELSE 'Unpaid'\n",
    "    END as payment_status\n",
    "FROM postgresql.public.trn_orders o\n",
    "LEFT JOIN mysql.demo_db.trn_payments p ON o.order_id = p.order_id\n",
    "ORDER BY o.order_ts DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "cross_catalog_df = execute_sql_query(conn, cross_catalog_query)\n",
    "display(cross_catalog_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully executed cross-catalog query returning {len(cross_catalog_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Data Access Verification\n",
    "\n",
    "Verify that we can access all required tables and get row counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data access across all catalogs\n",
    "print(\"üìä Data Access Verification:\")\n",
    "data_access_results = test_data_access(conn)\n",
    "\n",
    "print(\"\\nüìà Table Row Counts:\")\n",
    "print(\"=\" * 50)\n",
    "total_rows = 0\n",
    "for table, count in data_access_results.items():\n",
    "    if count >= 0:\n",
    "        print(f\"‚úÖ {table:<35}: {count:,} rows\")\n",
    "        total_rows += count\n",
    "    else:\n",
    "        print(f\"‚ùå {table:<35}: Access failed\")\n",
    "\n",
    "print(f\"\\nüìä Total rows across all tables: {total_rows:,}\")\n",
    "print(\"\\n‚úÖ Level 1 completed: Connections and catalog exploration successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Level 2: Data Aggregation and DataFrame Creation\n",
    "\n",
    "In this level, we perform data aggregation across multiple data sources and create analytical DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Daily Orders Aggregation\n",
    "\n",
    "Aggregate order data by day to calculate daily revenue and order counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily orders from PostgreSQL\n",
    "print(\"üì¶ Aggregating daily orders data...\")\n",
    "orders_df = aggregate_daily_orders(conn)\n",
    "\n",
    "print(f\"\\nüìä Daily Orders Summary:\")\n",
    "print(f\"  ‚Ä¢ Date range: {orders_df['dt'].min()} to {orders_df['dt'].max()}\")\n",
    "print(f\"  ‚Ä¢ Total days: {len(orders_df)}\")\n",
    "print(f\"  ‚Ä¢ Total revenue: ${orders_df['revenue'].sum():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Total orders: {orders_df['orders_cnt'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Average daily revenue: ${orders_df['revenue'].mean():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Average daily orders: {orders_df['orders_cnt'].mean():.1f}\")\n",
    "\n",
    "print(\"\\nüìã Sample Daily Orders Data:\")\n",
    "display(orders_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Daily Payments Aggregation\n",
    "\n",
    "Aggregate payment data by day to calculate daily payment amounts and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily payments from MySQL\n",
    "print(\"üí≥ Aggregating daily payments data...\")\n",
    "payments_df = aggregate_daily_payments(conn)\n",
    "\n",
    "print(f\"\\nüìä Daily Payments Summary:\")\n",
    "print(f\"  ‚Ä¢ Date range: {payments_df['dt'].min()} to {payments_df['dt'].max()}\")\n",
    "print(f\"  ‚Ä¢ Total days: {len(payments_df)}\")\n",
    "print(f\"  ‚Ä¢ Total paid amount: ${payments_df['paid_amount'].sum():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Total payments: {payments_df['payments_cnt'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Average daily paid amount: ${payments_df['paid_amount'].mean():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Average daily payments: {payments_df['payments_cnt'].mean():.1f}\")\n",
    "\n",
    "print(\"\\nüìã Sample Daily Payments Data:\")\n",
    "display(payments_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DataFrame Merging with Missing Value Handling\n",
    "\n",
    "Merge orders and payments DataFrames using proper handling of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames with proper fillna handling\n",
    "print(\"üîó Merging orders and payments DataFrames...\")\n",
    "merged_df = merge_dataframes_with_fillna(orders_df, payments_df, fill_value=0.0)\n",
    "\n",
    "print(f\"\\nüìä Merged DataFrame Summary:\")\n",
    "print(f\"  ‚Ä¢ Total rows: {len(merged_df)}\")\n",
    "print(f\"  ‚Ä¢ Date range: {merged_df['dt'].min()} to {merged_df['dt'].max()}\")\n",
    "print(f\"  ‚Ä¢ Columns: {list(merged_df.columns)}\")\n",
    "\n",
    "# Check for missing values after merge\n",
    "missing_values = merged_df.isnull().sum()\n",
    "print(f\"\\nüîç Missing values after merge:\")\n",
    "for col, count in missing_values.items():\n",
    "    if count > 0:\n",
    "        print(f\"  ‚Ä¢ {col}: {count} missing values\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ {col}: No missing values\")\n",
    "\n",
    "print(\"\\nüìã Sample Merged Data:\")\n",
    "display(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Payment Coverage Calculation\n",
    "\n",
    "Calculate payment coverage metrics to understand the relationship between orders and payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate payment coverage metrics\n",
    "print(\"üìà Calculating payment coverage metrics...\")\n",
    "coverage_df = calculate_payment_coverage(merged_df)\n",
    "\n",
    "print(f\"\\nüìä Payment Coverage Analysis:\")\n",
    "coverage_stats = coverage_df['payment_coverage'].describe()\n",
    "print(f\"  ‚Ä¢ Average coverage: {coverage_stats['mean']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Median coverage: {coverage_stats['50%']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Maximum coverage: {coverage_stats['max']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Minimum coverage: {coverage_stats['min']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Standard deviation: {coverage_stats['std']:.2%}\")\n",
    "\n",
    "# Coverage distribution analysis\n",
    "full_coverage_days = (coverage_df['payment_coverage'] >= 1.0).sum()\n",
    "high_coverage_days = (coverage_df['payment_coverage'] >= 0.8).sum()\n",
    "low_coverage_days = (coverage_df['payment_coverage'] < 0.5).sum()\n",
    "\n",
    "print(f\"\\nüìä Coverage Distribution:\")\n",
    "print(f\"  ‚Ä¢ Days with full coverage (‚â•100%): {full_coverage_days} ({full_coverage_days/len(coverage_df):.1%})\")\n",
    "print(f\"  ‚Ä¢ Days with high coverage (‚â•80%): {high_coverage_days} ({high_coverage_days/len(coverage_df):.1%})\")\n",
    "print(f\"  ‚Ä¢ Days with low coverage (<50%): {low_coverage_days} ({low_coverage_days/len(coverage_df):.1%})\")\n",
    "\n",
    "print(\"\\nüìã Sample Coverage Data:\")\n",
    "display(coverage_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Complete Analytics DataFrame Creation\n",
    "\n",
    "Create the final analytics DataFrame using the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final analytics DataFrame using complete pipeline\n",
    "print(\"üèóÔ∏è Creating final analytics DataFrame...\")\n",
    "final_df = create_final_analytics_dataframe(conn)\n",
    "\n",
    "print(f\"\\nüìä Final Analytics DataFrame:\")\n",
    "print(f\"  ‚Ä¢ Shape: {final_df.shape}\")\n",
    "print(f\"  ‚Ä¢ Columns: {list(final_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Date range: {final_df['dt'].min()} to {final_df['dt'].max()}\")\n",
    "\n",
    "print(\"\\nüìã Final DataFrame Sample:\")\n",
    "display(final_df.head(10))\n",
    "\n",
    "print(\"\\nüìã Final DataFrame Tail:\")\n",
    "display(final_df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Analytics Summary Generation\n",
    "\n",
    "Generate comprehensive summary statistics for the analytics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analytics summary\n",
    "print(\"üìà Generating analytics summary...\")\n",
    "summary = get_analytics_summary(final_df)\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE ANALYTICS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìÖ Time Period:\")\n",
    "print(f\"  ‚Ä¢ Total days analyzed: {summary['total_days']}\")\n",
    "print(f\"  ‚Ä¢ Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
    "\n",
    "print(f\"\\nüí∞ Revenue Analysis:\")\n",
    "print(f\"  ‚Ä¢ Total revenue: ${summary['revenue']['total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Average daily revenue: ${summary['revenue']['average_daily']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Maximum daily revenue: ${summary['revenue']['max_daily']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Minimum daily revenue: ${summary['revenue']['min_daily']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüì¶ Orders Analysis:\")\n",
    "print(f\"  ‚Ä¢ Total orders: {summary['orders']['total']:,}\")\n",
    "print(f\"  ‚Ä¢ Average daily orders: {summary['orders']['average_daily']:.1f}\")\n",
    "print(f\"  ‚Ä¢ Maximum daily orders: {summary['orders']['max_daily']:,}\")\n",
    "print(f\"  ‚Ä¢ Minimum daily orders: {summary['orders']['min_daily']:,}\")\n",
    "\n",
    "print(f\"\\nüí≥ Payments Analysis:\")\n",
    "print(f\"  ‚Ä¢ Total payments: {summary['payments']['total']:,}\")\n",
    "print(f\"  ‚Ä¢ Total paid amount: ${summary['payments']['total_amount']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Average daily payment count: {summary['payments']['average_daily_count']:.1f}\")\n",
    "print(f\"  ‚Ä¢ Average daily paid amount: ${summary['payments']['average_daily_amount']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìà Payment Coverage Analysis:\")\n",
    "print(f\"  ‚Ä¢ Average coverage: {summary['payment_coverage']['average']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Maximum coverage: {summary['payment_coverage']['max']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Minimum coverage: {summary['payment_coverage']['min']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Days with full coverage: {summary['payment_coverage']['days_with_full_coverage']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Level 2 completed: Data aggregation and DataFrame creation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Level 3: Visualization and Iceberg Storage\n",
    "\n",
    "In this level, we create professional visualizations and save the analytics results to Apache Iceberg for long-term storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Visualization Preparation\n",
    "\n",
    "Prepare the data for visualization and display summary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display chart data summary\n",
    "print(\"üìä Preparing data for visualization...\")\n",
    "display_chart_summary(final_df)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\n‚úÖ Data prepared for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Time-Series Revenue Visualization\n",
    "\n",
    "Create a professional time-series chart showing daily revenue trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-series revenue chart\n",
    "print(\"üìà Creating time-series revenue chart...\")\n",
    "fig1 = create_time_series_revenue_chart(\n",
    "    final_df,\n",
    "    figsize=(14, 8),\n",
    "    title=\"Daily Revenue Analysis - Time Series Trend\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Time-series revenue chart created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Payment Coverage Distribution\n",
    "\n",
    "Create a histogram showing the distribution of payment coverage ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment coverage histogram\n",
    "print(\"üìä Creating payment coverage distribution histogram...\")\n",
    "fig2 = create_payment_coverage_histogram(\n",
    "    final_df,\n",
    "    bins=25,\n",
    "    figsize=(12, 8),\n",
    "    title=\"Payment Coverage Distribution Analysis\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Payment coverage histogram created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Comprehensive Analytics Dashboard\n",
    "\n",
    "Create a comprehensive dashboard combining multiple visualizations and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analytics dashboard\n",
    "print(\"üéõÔ∏è Creating comprehensive analytics dashboard...\")\n",
    "fig3 = create_combined_analytics_dashboard(\n",
    "    final_df,\n",
    "    figsize=(18, 12),\n",
    "    title=\"Trino Analytics Homework - Comprehensive Dashboard\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comprehensive analytics dashboard created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Iceberg Storage - Schema and Table Creation\n",
    "\n",
    "Save the analytics results to Apache Iceberg for long-term storage and future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analytics data to Iceberg\n",
    "print(\"üèîÔ∏è Saving analytics data to Apache Iceberg...\")\n",
    "iceberg_results = save_analytics_to_iceberg(\n",
    "    conn, \n",
    "    final_df, \n",
    "    table_name=\"trino_homework_analytics\",\n",
    "    schema_name=\"homework\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Iceberg Storage Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"‚úÖ Success: {iceberg_results['success']}\")\n",
    "print(f\"üìã Table: {iceberg_results['full_table_name']}\")\n",
    "print(f\"üìä Rows inserted: {iceberg_results['rows_inserted']:,}\")\n",
    "print(f\"üîç Verification passed: {iceberg_results['verification']['verification_passed']}\")\n",
    "print(f\"‚è∞ Timestamp: {iceberg_results['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if iceberg_results['success']:\n",
    "    print(\"\\nüéâ Data successfully saved to Iceberg!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Iceberg storage completed with issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Iceberg Data Verification\n",
    "\n",
    "Verify that the data was correctly persisted to Iceberg by querying it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data persistence in Iceberg\n",
    "print(\"üîç Verifying data persistence in Iceberg...\")\n",
    "\n",
    "# Query data back from Iceberg\n",
    "iceberg_df = query_iceberg_table(\n",
    "    conn, \n",
    "    \"trino_homework_analytics\", \n",
    "    \"homework\",\n",
    "    order_by=\"dt\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Iceberg Query Results:\")\n",
    "print(f\"  ‚Ä¢ Rows retrieved: {len(iceberg_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {list(iceberg_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Date range: {iceberg_df['dt'].min()} to {iceberg_df['dt'].max()}\")\n",
    "\n",
    "# Compare key metrics\n",
    "print(f\"\\nüîç Data Integrity Check:\")\n",
    "original_revenue = final_df['revenue'].sum()\n",
    "iceberg_revenue = iceberg_df['revenue'].sum()\n",
    "revenue_match = abs(original_revenue - iceberg_revenue) < 0.01\n",
    "\n",
    "original_orders = final_df['orders_cnt'].sum()\n",
    "iceberg_orders = iceberg_df['orders_cnt'].sum()\n",
    "orders_match = original_orders == iceberg_orders\n",
    "\n",
    "print(f\"  {'‚úÖ' if revenue_match else '‚ùå'} Revenue: Original=${original_revenue:,.2f}, Iceberg=${iceberg_revenue:,.2f}\")\n",
    "print(f\"  {'‚úÖ' if orders_match else '‚ùå'} Orders: Original={original_orders:,}, Iceberg={iceberg_orders:,}\")\n",
    "print(f\"  {'‚úÖ' if len(final_df) == len(iceberg_df) else '‚ùå'} Row count: Original={len(final_df)}, Iceberg={len(iceberg_df)}\")\n",
    "\n",
    "print(\"\\nüìã Sample Iceberg Data:\")\n",
    "display(iceberg_df.head(5))\n",
    "\n",
    "if revenue_match and orders_match and len(final_df) == len(iceberg_df):\n",
    "    print(\"\\n‚úÖ Data integrity verification passed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Data integrity verification found discrepancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Iceberg Table Management\n",
    "\n",
    "Demonstrate Iceberg table management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Iceberg tables in the homework schema\n",
    "print(\"üìö Listing Iceberg tables...\")\n",
    "iceberg_tables = list_iceberg_tables(conn, \"homework\")\n",
    "\n",
    "if not iceberg_tables.empty:\n",
    "    print(f\"\\nüìä Found {len(iceberg_tables)} table(s) in homework schema:\")\n",
    "    for table in iceberg_tables['table_name']:\n",
    "        print(f\"  ‚Ä¢ {table}\")\n",
    "    display(iceberg_tables)\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No tables found in homework schema\")\n",
    "\n",
    "# Demonstrate analytical query on Iceberg data\n",
    "print(\"\\nüìà Running analytical query on Iceberg data...\")\n",
    "iceberg_analytics = execute_sql_query(conn, \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_days,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        AVG(revenue) as avg_daily_revenue,\n",
    "        SUM(orders_cnt) as total_orders,\n",
    "        AVG(payment_coverage) as avg_coverage,\n",
    "        COUNT(CASE WHEN payment_coverage >= 1.0 THEN 1 END) as full_coverage_days\n",
    "    FROM iceberg.homework.trino_homework_analytics\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Iceberg Analytics Summary:\")\n",
    "display(iceberg_analytics)\n",
    "\n",
    "print(\"\\n‚úÖ Iceberg table management demonstration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary and Cleanup\n",
    "\n",
    "Summarize the complete workflow and clean up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "This notebook successfully demonstrated a complete analytics workflow using Trino for federated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final workflow summary\n",
    "print(\"üéØ TRINO ANALYTICS HOMEWORK - FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ LEVEL 1 - Connections and Catalog Exploration:\")\n",
    "print(\"  ‚Ä¢ Established connection to Trino coordinator\")\n",
    "print(\"  ‚Ä¢ Verified connectivity to PostgreSQL, MySQL, and Iceberg catalogs\")\n",
    "print(\"  ‚Ä¢ Explored table structures and sample data\")\n",
    "print(\"  ‚Ä¢ Demonstrated cross-catalog federated queries\")\n",
    "\n",
    "print(\"\\n‚úÖ LEVEL 2 - Data Aggregation and DataFrame Creation:\")\n",
    "print(\"  ‚Ä¢ Aggregated daily orders data from PostgreSQL\")\n",
    "print(\"  ‚Ä¢ Aggregated daily payments data from MySQL\")\n",
    "print(\"  ‚Ä¢ Merged DataFrames with proper missing value handling\")\n",
    "print(\"  ‚Ä¢ Calculated payment coverage metrics\")\n",
    "print(\"  ‚Ä¢ Generated comprehensive analytics summary\")\n",
    "\n",
    "print(\"\\n‚úÖ LEVEL 3 - Visualization and Iceberg Storage:\")\n",
    "print(\"  ‚Ä¢ Created professional time-series revenue charts\")\n",
    "print(\"  ‚Ä¢ Generated payment coverage distribution histograms\")\n",
    "print(\"  ‚Ä¢ Built comprehensive analytics dashboard\")\n",
    "print(\"  ‚Ä¢ Saved results to Apache Iceberg for long-term storage\")\n",
    "print(\"  ‚Ä¢ Verified data persistence and integrity\")\n",
    "\n",
    "print(f\"\\nüìä KEY METRICS:\")\n",
    "print(f\"  ‚Ä¢ Total days analyzed: {len(final_df)}\")\n",
    "print(f\"  ‚Ä¢ Total revenue: ${final_df['revenue'].sum():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Total orders: {final_df['orders_cnt'].sum():,}\")\n",
    "print(f\"  ‚Ä¢ Average payment coverage: {final_df['payment_coverage'].mean():.2%}\")\n",
    "print(f\"  ‚Ä¢ Data successfully persisted to Iceberg: {'Yes' if iceberg_results['success'] else 'No'}\")\n",
    "\n",
    "print(f\"\\n‚è∞ Execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüéâ HOMEWORK COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Clean up connections and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Trino connection\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "close_connection(conn)\n",
    "print(\"‚úÖ Trino connection closed successfully\")\n",
    "\n",
    "# Clear large variables to free memory\n",
    "del final_df, orders_df, payments_df, merged_df, coverage_df, iceberg_df\n",
    "print(\"‚úÖ Memory cleanup completed\")\n",
    "\n",
    "print(\"\\nüèÅ All resources cleaned up successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Thank you for using Trino Analytics Homework!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}